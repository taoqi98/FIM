{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from collections import Counter\n",
    "import re\n",
    "import json\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "root_data_path = None\n",
    "root_path = root_data_path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "from models import *\n",
    "from DataGenerator import *\n",
    "from utils import *\n",
    "from DataGenerator import *\n",
    "from preprocess import *\n",
    "from Encoder import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "os. environ[\"CUDA_VISIBLE_DEVICES\"] = \"0\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "MAX_ENTITY_NUM = 5\n",
    "\n",
    "MAX_TITLE_LEN = 30\n",
    "MAX_SNI_LEN=100\n",
    "MAX_VERT_NUM=3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "attr = ['title',]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "news,news_index,category_dict,word_dict,entity_dict = read_news(root_data_path,attr,'entity/News_entity_title.tsv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "news_title, news_vert, news_sni, news_entity=get_doc_input(attr,news, news_index,category_dict,word_dict,entity_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "class NewsFetcher():\n",
    "    def __init__(self,attr,news_title, news_vert, news_sni,news_entity,onehop):\n",
    "        self.title = news_title\n",
    "        self.sni = news_sni\n",
    "        self.vert = news_vert\n",
    "        self.entity = news_entity\n",
    "        self.onehop = onehop\n",
    "        \n",
    "        self.attr = attr\n",
    "    \n",
    "    def fetch_news(self,docids):\n",
    "        attr = self.attr\n",
    "        if 'title' in attr:\n",
    "            title = self.title[docids]\n",
    "        else:\n",
    "            title = None\n",
    "        if 'vert' in attr:\n",
    "            vert = self.vert[docids]\n",
    "        else:\n",
    "            vert = None\n",
    "        if 'sni' in attr:\n",
    "            sni = self.sni[docids]\n",
    "        else:\n",
    "            sni = None\n",
    "        if 'entity' in attr:\n",
    "            entity = self.entity[docids]\n",
    "            onehop = self.onehop[entity]\n",
    "\n",
    "            entity = entity.reshape(list(entity.shape)+[1])\n",
    "            entity = np.concatenate([entity,onehop],axis=-1)\n",
    "            if len(entity.shape) == 4:\n",
    "                entity = entity.reshape((entity.shape[0],entity.shape[1],-1))\n",
    "            else:\n",
    "                entity = entity.reshape((entity.shape[0],-1))\n",
    "            #print(entity.shape,onehop.shape)\n",
    "            \n",
    "            return [entity_matrix[entity]]\n",
    "            \n",
    "        else:\n",
    "            entity = None\n",
    "        \n",
    "        feature = [title,]\n",
    "        if entity in attr:\n",
    "            feature.append(entity)\n",
    "        \n",
    "#         for a in attr:\n",
    "#             feature.append(Table[a])\n",
    "#         if len(feature) == 1:\n",
    "#             feature = feature[0]\n",
    "#         else:\n",
    "#             feature = np.concatenate(feature,axis=-1)\n",
    "        \n",
    "        feature = np.concatenate(feature,axis=-1)\n",
    " \n",
    "        return [feature]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "if 'entity' in attr:\n",
    "    entity_matrix = load_entity_matrix(root_data_path,entity_dict,)\n",
    "else:\n",
    "    entity_matrix = None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "news_fetcher = NewsFetcher(attr,news_title,news_vert,news_sni,news_entity,None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "embedding_path = None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "finish user and sessions\n"
     ]
    }
   ],
   "source": [
    "TrainUserInfo,Train_UserId2Index,Train_UserIndex2Id = parse_train_user(news_index,root_path,'train_users.json')\n",
    "TestUserInfo,Test_UserId2Index,Test_UserIndex2Id = parse_train_user(news_index,root_path,'test_users.json')\n",
    "train_docids, train_labels, train_userindex,train_time = parse_train_samples(news_index,Train_UserId2Index,root_path,'train_samples.json')\n",
    "test_impressions = parse_test_impression(news_index,root_path,'test_sessions.json')\n",
    "print('finish user and sessions')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "embedding_matrix, have_word = load_matrix(embedding_path,word_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_generator = get_hir_train_generator(news_fetcher,TrainUserInfo,Train_UserIndex2Id,train_docids,train_userindex,train_time,train_labels,32)\n",
    "test_user_generator = get_hir_user_generator(news_fetcher,TestUserInfo,test_impressions,16)\n",
    "news_generator = get_hir_news_generator(news_fetcher,news_title,16)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_entity_matrix(root_data_path,entity_dict):\n",
    "    \n",
    "    with open(os.path.join(root_data_path,'entity','entity2id.txt')) as f:\n",
    "        lines = f.readlines()\n",
    "\n",
    "    satorid2line = {}\n",
    "\n",
    "    for i in range(1,len(lines)):\n",
    "        sid, index = lines[i].strip('\\n').split('\\t')\n",
    "        index = int(index)\n",
    "        satorid2line[sid] = index\n",
    "\n",
    "    with open(os.path.join(root_data_path,'entity','entity2vec.vec')) as f:\n",
    "        lines = f.readlines()\n",
    "        \n",
    "    stat = 0\n",
    "    entity_matrix = np.zeros((len(entity_dict)+1,100))\n",
    "    for e in entity_dict:\n",
    "        if not sid in satorid2line:\n",
    "            stat += 1\n",
    "            continue\n",
    "        eindex = entity_dict[e]\n",
    "        index = satorid2line[sid]\n",
    "        line = lines[index]\n",
    "        line = line.strip('\\n').split('\\t')[:-1]\n",
    "        for j in range(len(line)):\n",
    "            line[j] = float(line[j])\n",
    "            entity_matrix[eindex,j] = line[j]\n",
    "            \n",
    "    print(stat/len(entity_dict))\n",
    "    \n",
    "    return entity_matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "def parse_test():\n",
    "    docids = []\n",
    "    userids = []\n",
    "    labels = []\n",
    "    eventimes = []\n",
    "    \n",
    "    locs = []\n",
    "    \n",
    "    for i in range(len(test_impressions)):\n",
    "        start = len(docids)\n",
    "        doc,label,uid, eventime =  test_impressions[i]\n",
    "        docids+=list(doc)\n",
    "        labels+=list(label)\n",
    "        uid = Test_UserId2Index[uid]\n",
    "        userids +=[uid]*len(label)\n",
    "        eventimes += [eventime]*len(label)\n",
    "        ed = len(docids)\n",
    "        locs.append([start,ed])\n",
    "        \n",
    "    docids = np.array(docids)\n",
    "    userids = np.array(userids)\n",
    "    eventimes = np.array(eventimes)\n",
    "    labels = np.array(labels)\n",
    "\n",
    "    return docids, userids, eventimes, labels,locs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_docids, test_userids, test_eventimes, test_labels, test_locs = parse_test()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "class get_test_generator(Sequence):\n",
    "    def __init__(self,news_fetcher,user_info,UserIndex2Id, news_id,user_id,imp_time, label, batch_size):\n",
    "        \n",
    "        self.news_fetcher = news_fetcher\n",
    "        \n",
    "        self.user_info = user_info\n",
    "        self.UserIndex2Id = UserIndex2Id\n",
    "        \n",
    "        self.doc_id = news_id\n",
    "        self.user_id = user_id\n",
    "        self.imp_time = imp_time\n",
    "        self.label = label\n",
    "        \n",
    "        self.batch_size = batch_size\n",
    "        self.Imp = label.shape[0]\n",
    "        \n",
    "    def __len__(self):\n",
    "        return int(np.ceil(self.Imp/self.batch_size))\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        start = idx*self.batch_size\n",
    "        ed = (idx+1)*self.batch_size\n",
    "        if ed >self.Imp:\n",
    "            ed = self.Imp\n",
    "        \n",
    "        doc_ids = self.doc_id[start:ed]\n",
    "        news_feature = self.news_fetcher.fetch_news(doc_ids)\n",
    "        \n",
    "        clicked_ids = []\n",
    "        for i in range(start,ed):\n",
    "            uindex = self.user_id[i]\n",
    "            uid = self.UserIndex2Id[uindex]\n",
    "            user_click, user_eventime = self.user_info[uid]\n",
    "            eventime = self.imp_time[i]\n",
    "            uc = user_click.copy()\n",
    "            ct = (user_eventime<eventime).sum()\n",
    "            if ct > 50:\n",
    "                uc = uc[ct-50:ct]\n",
    "            else:\n",
    "                uc = [0]*(50-ct) + uc[:ct]\n",
    "            clicked_ids.append(uc)\n",
    "        clicked_ids = np.array(clicked_ids)\n",
    "        user_feature = self.news_fetcher.fetch_news(clicked_ids)\n",
    "        \n",
    "        label = self.label[start:ed]\n",
    "                \n",
    "        return (news_feature+user_feature,[label])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_generator = get_test_generator(news_fetcher,TestUserInfo,Test_UserIndex2Id,test_docids,test_userids,test_eventimes,test_labels[:1250000],32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate(plabel,labels,locs):\n",
    "    AUC = []\n",
    "    MRR = []\n",
    "    nDCG5 = []\n",
    "    nDCG10 =[]\n",
    "    for i in range(len(locs)):\n",
    "        start,ed=locs[i]\n",
    "        score = plabel[start:ed]\n",
    "        label = labels[start:ed]\n",
    "        \n",
    "\n",
    "        auc = roc_auc_score(label,score)\n",
    "        mrr = mrr_score(label,score)\n",
    "        ndcg5 = ndcg_score(label,score,k=5)\n",
    "        ndcg10 = ndcg_score(label,score,k=10)\n",
    "    \n",
    "        AUC.append(auc)\n",
    "        MRR.append(mrr)\n",
    "        nDCG5.append(ndcg5)\n",
    "        nDCG10.append(ndcg10)\n",
    "    AUC = np.array(AUC)\n",
    "    MRR = np.array(MRR)\n",
    "    nDCG5 = np.array(nDCG5)\n",
    "    nDCG10 = np.array(nDCG10)\n",
    "    \n",
    "    AUC = AUC.mean()\n",
    "    MRR = MRR.mean()\n",
    "    nDCG5 = nDCG5.mean()\n",
    "    nDCG10 = nDCG10.mean()\n",
    "    \n",
    "    return AUC, MRR, nDCG5, nDCG10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "def CNN_emb_extractor(length,word_dict,title_word_embedding_matrix):\n",
    "\n",
    "    sentence_input = Input(shape=(length,), dtype='int32')\n",
    "    title_word_embedding_layer = Embedding(len(word_dict)+1, 300, weights=[title_word_embedding_matrix],trainable=True)\n",
    "    \n",
    "    \n",
    "    word_vecs0 = title_word_embedding_layer(sentence_input)\n",
    "    word_vecs1 = Conv1D(300,kernel_size=3,activation='relu',padding='same')(word_vecs0)\n",
    "    word_vecs2 = Conv1D(300,kernel_size=3,activation='relu',dilation_rate=2,padding='same')(word_vecs1)\n",
    "    word_vecs3 = Conv1D(300,kernel_size=3,activation='relu',dilation_rate=3,padding='same')(word_vecs2)\n",
    "        \n",
    "    word_vecs0 = keras.layers.Reshape((1,length,300))(word_vecs0)\n",
    "    word_vecs1 = keras.layers.Reshape((1,length,300))(word_vecs1)\n",
    "    word_vecs2 = keras.layers.Reshape((1,length,300))(word_vecs2)\n",
    "    word_vecs3 = keras.layers.Reshape((1,length,300))(word_vecs3)\n",
    "\n",
    "    \n",
    "    word_vecs = keras.layers.Concatenate(axis=-3)([word_vecs0,word_vecs1,word_vecs2,word_vecs3])\n",
    "    word_vecs = Dropout(0.2)(word_vecs)\n",
    "    model = Model(sentence_input,word_vecs)\n",
    "    \n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def DotLayer():\n",
    "    import math\n",
    "    fs = math.sqrt(300)\n",
    "    vecs_input = Input(shape=(30,600))\n",
    "    user_vecs = keras.layers.Lambda(lambda x:x[:,:,:300])(vecs_input)\n",
    "    news_vecs = keras.layers.Lambda(lambda x:x[:,:,300:])(vecs_input)\n",
    "    inter = keras.layers.Dot(axes=-1)([user_vecs,news_vecs])\n",
    "    inter = keras.layers.Lambda(lambda x:x/fs)(inter)\n",
    "    return Model(vecs_input,inter)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_interaction_model(): #with title\n",
    "    \n",
    "    user_input = Input(shape=(50,4,30,300))\n",
    "    news_input = Input(shape=(4,30,300))\n",
    "    \n",
    "    news_vecs = keras.layers.Reshape((4*30*300,))(news_input)\n",
    "    news_vecs = RepeatVector(50)(news_vecs)\n",
    "    news_vecs = keras.layers.Reshape((50,4,30,300))(news_vecs)\n",
    "    \n",
    "    vecs = keras.layers.Concatenate(axis=-1)([user_input,news_vecs])\n",
    "    dot_layer = DotLayer()\n",
    "    \n",
    "    inter_emb = TimeDistributed(TimeDistributed(dot_layer))(vecs)\n",
    "    inter_vecs = Conv3D(300,kernel_size=(3,3,3),padding='same',activation='relu')(inter_emb)\n",
    "    \n",
    "    inter_vec = GlobalMaxPooling3D()(inter_vecs)\n",
    "    \n",
    "    vec = Dense(128,activation='relu')(inter_vec)\n",
    "    vec = Dense(128,activation='relu')(vec)\n",
    "    score = Dense(1)(vec)\n",
    "    \n",
    "    return Model([user_input,news_input,],score)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_model(title_word_embedding_matrix): #with title\n",
    "    \n",
    "    emb_extractor = CNN_emb_extractor(30,word_dict,title_word_embedding_matrix)\n",
    "    inter_model = create_interaction_model()\n",
    "    \n",
    "    clicked_title_input = Input(shape=(50,30), dtype='int32')\n",
    "    clicked_entity_input = Input(shape=(50,max_entity_num,100))\n",
    "    clicked_one_hop_input = Input(shape=(50,max_entity_num,max_entity_num,100))\n",
    "    \n",
    "    title_inputs = Input(shape=(1+npratio,30,),dtype='int32')\n",
    "    entity_inputs = Input(shape=(1+npratio,max_entity_num,100),dtype='float32')\n",
    "    one_hop_inputs = Input(shape=(1+npratio,max_entity_num,max_entity_num,100),dtype='float32')\n",
    "\n",
    "\n",
    "    user_emb = TimeDistributed(emb_extractor)(clicked_title_input)\n",
    "    news_emb = TimeDistributed(emb_extractor)(title_inputs)\n",
    "    news_embs = [keras.layers.Lambda(lambda x:x[:,i,])(news_emb) for i in range(1+npratio)]\n",
    "    \n",
    "    scores = []\n",
    "    for i in range(1+npratio):\n",
    "        score = inter_model([user_emb,news_embs[i]])\n",
    "        scores.append(score)\n",
    "    \n",
    "    scores = keras.layers.Concatenate(axis=-1)(scores)\n",
    "    logit = keras.layers.Activation('softmax')(scores)\n",
    "    \n",
    "    model = Model([clicked_title_input,title_inputs,],logit)\n",
    "    \n",
    "    model.compile(loss=['categorical_crossentropy'],\n",
    "                  optimizer=Adam(lr=0.0001), \n",
    "                  metrics=['acc'])\n",
    "    \n",
    "    \n",
    "    one_title_input = Input(shape=(30,), dtype='int32')\n",
    "    one_entity_input = Input(shape=(max_entity_num,100))\n",
    "    one_one_hop_input = Input(shape=(max_entity_num,max_entity_num,100))\n",
    "    \n",
    "    one_emb = emb_extractor(one_title_input)\n",
    "    one_score = inter_model([user_emb,one_emb])\n",
    "    scoring_model = Model([one_title_input,clicked_title_input,],one_score)\n",
    "    \n",
    "    \n",
    "    return model,scoring_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/1\n",
      "7448/7448 [==============================] - 2945s 395ms/step - loss: 0.6084 - acc: 0.6629\n",
      "39063/39063 [==============================] - 2989s 77ms/step\n",
      "Epoch 1/1\n",
      "7448/7448 [==============================] - 2876s 386ms/step - loss: 0.6090 - acc: 0.6608\n",
      "39063/39063 [==============================] - 2978s 76ms/step\n",
      "Epoch 1/1\n",
      "7389/7448 [============================>.] - ETA: 22s - loss: 0.6096 - acc: 0.6631"
     ]
    }
   ],
   "source": [
    "for i in range(10):\n",
    "    model,inter_model = create_model(embedding_matrix)\n",
    "    model.fit_generator(train_generator)\n",
    "    plabel = inter_model.predict_generator(test_generator,verbose=1)\n",
    "    g = evaluate(plabel[:,0],test_labels, test_locs[:20000])\n",
    "    s = json.dumps(g)\n",
    "    with open('FIM.json','a') as f:\n",
    "        f.write(s+'\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
